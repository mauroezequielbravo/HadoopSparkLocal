version: '3.8'

services:
  # =====================================================
  # Contenedor de Hadoop (HDFS + YARN)
  # =====================================================
  hadoop:
    build:
      context: ./hadoop
      dockerfile: Dockerfile
    container_name: hadoop-master
    hostname: hadoop-master
    networks:
      - hadoop-spark-net
    ports:
      # HDFS NameNode UI
      - "9870:9870"
      # HDFS DataNode HTTP
      - "9864:9864"
      # YARN ResourceManager UI
      - "8088:8088"
      # YARN NodeManager Web UI
      - "8042:8042"
    volumes:
      - ./hadoop-data:/opt/hadoop/data
      - ./hadoop/config/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml:ro
      - ./hadoop/config/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml:ro
      - ./hadoop/config/mapred-site.xml:/opt/hadoop/etc/hadoop/mapred-site.xml:ro
      - ./hadoop/config/yarn-site.xml:/opt/hadoop/etc/hadoop/yarn-site.xml:ro
      - ./hadoop/config/hadoop-env.sh:/opt/hadoop/etc/hadoop/hadoop-env.sh:ro
    environment:
      - HDFS_NAMENODE_USER=root
      - HDFS_DATANODE_USER=root
      - HDFS_SECONDARYNAMENODE_USER=root
      - YARN_RESOURCEMANAGER_USER=root
      - YARN_NODEMANAGER_USER=root
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9870" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  # =====================================================
  # Contenedor de Spark + Jupyter
  # =====================================================
  spark:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-master
    hostname: spark-master
    networks:
      - hadoop-spark-net
    ports:
      # Jupyter Notebook
      - "8888:8888"
      # Spark Master UI
      - "8080:8080"
      # Spark Worker UI
      - "8081:8081"
      # Spark Application UI
      - "4040:4040"
    volumes:
      - ./notebooks:/notebooks
      - ./hadoop/config/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml:ro
      - ./hadoop/config/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml:ro
    environment:
      - PYSPARK_ALLOW_INSECURE_GATEWAY=1
      - SPARK_MASTER_HOST=spark-master
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    depends_on:
      hadoop:
        condition: service_healthy
    restart: unless-stopped

# =====================================================
# Red personalizada para comunicaci√≥n entre contenedores
# =====================================================
networks:
  hadoop-spark-net:
    driver: bridge
    name: hadoop-spark-network
